# @package _global_

# To run this test do python3 slurm.py experiment=env1_augmentation ...
defaults:
  - override /model: mlp
  - override /lr_scheduler: cosine
  - override /optimizer: adamw
  - override /trainer: trainer_mlp

name: 'mlp'

model:
  hidden_size: 256
  depth: 8
  num_heads: 8
  mlp_ratio: 4.0
  max_transitions: 20
  dropout: 0.0

train_dataloader:
  batch_size: 1024
  num_workers: 6
  dataset:
    max_tokens: 20

val_dataloader:
  batch_size: 256
  num_workers: 6
  shuffle: False
  dataset:
    max_tokens: 20

trainer: 
  iterations: 30000
  validate_every: 1000
  patience: null
  use_ema: true

wandb:
  mode: online